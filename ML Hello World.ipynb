{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mnist digit recognition is the very first algorithm to write for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('data/mnist_data/',one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what was downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls data/mnist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the data and see what's in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_digits,training_labels = mnist.train.next_batch(5000)\n",
    "test_digits, test_labels = mnist.test.next_batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_digits.shape)\n",
    "print(training_labels.shape)\n",
    "print(training_digits,training_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data will be used as inputs to the model , so let's create placeholders for them, as well as :\n",
    "1. Define the cost\n",
    "2. Create optimizer, that  minimizes this cost\n",
    "3. Batch up the training set and looping through it, while fitting the parameters W and b by running the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate=0.01\n",
    "batch_size=100\n",
    "display_step = 1\n",
    "mnist = input_data.read_data_sets('data/mnist_data/',one_hot=True)\n",
    "\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,784], name='X')\n",
    "Y=tf.placeholder(dtype=tf.float32,shape=[None,10], name='Y')\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]),name='W')\n",
    "b= tf.Variable(tf.zeros([10]),name='b')\n",
    "\n",
    "\n",
    "Z=tf.matmul(X,W) +b\n",
    "A=tf.nn.softmax(Z)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(A)))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_index=0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, batch_cost = sess.run([train_step, cost], feed_dict={X:batch_xs,Y:batch_ys}) \n",
    "        batch_index +=1\n",
    "        #print(\"Batch:\", '%04d' % (batch_index), \"cost=\", \"{:.9f}\".format(batch_cost))                        \n",
    "    print(\"Optimization Finished!\")     \n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(A, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: mnist.train.images, Y: mnist.train.labels}))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    image=mnist.train.images[600]\n",
    "    result=sess.run(tf.argmax(A,axis=1),feed_dict={X:image.reshape(1,784)})\n",
    "    print(\"The recognized digit is {}\".format(result))\n",
    "    plt.imshow(image.reshape(28,28))\n",
    "    \n",
    "writer = tf.summary.FileWriter('./tbout/mnist1',sess.graph)\n",
    "writer.close()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Cost Function\n",
    "\n",
    "We've got to abandon the Mean Squared cost function because it slows down learning. We want a cost function that that learns quickly when the model is a lot wrong and slows down when the model is not that wrong\n",
    "\n",
    "Here's the best explaination for this - \n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function\n",
    "\n",
    "Entropy - disorder, uncertainty. If there is no uncertainty there is no information, e.g. if it always is sunny, if the weather forecast says it's sunny - there is not much information gained from it. \n",
    "    \n",
    "    `Entropy = amount of information.`  \n",
    " \n",
    "\n",
    "\n",
    "E.g. \n",
    "\n",
    "Assuming a known distribution of AB - 100% A and 0% B, how much information do you get when I tell you it's A?\n",
    "    \n",
    "    You've always know it'll be A because the distribution is 100% A, so it's not a surprise it's A, therefore you get 0 bit of information\n",
    "    \n",
    "    \n",
    "Assuming a known distribution of AB - 50% A and 50% B, how much information do you get when I tell you it's A?\n",
    "    \n",
    "    It could have be A or B - if I predicted it I could only gotten it right half the time - I've got to ask one question before I am certain ( is it A or B ). So I've got 1 bit of information \n",
    "        \n",
    "    \n",
    "Assuming a known distribution of ABC&D - 25% of A, 25% of B, 25% of C and 25% D, \n",
    "    \n",
    "    I could only have get a right outcome for sure after 2 questions, e.g. \n",
    "    \n",
    "    1-Is it A or B? \n",
    "        Yes - is it A? \n",
    "              Yes - it's A \n",
    "              No - it's B\n",
    "        No - is it C?\n",
    "             Yes - it's C\n",
    "             No - it's D\n",
    "             \n",
    "    I get 2 bit of information.\n",
    "    \n",
    "Assuming a know weather distribution of 75% sunny and 25% rainy,\n",
    "\n",
    "    When it's forecasted that it's going to rain tomorrow, my uncertainty has been reduced by a factor of 4(1/0.25) or I get 2 bit of information also give by log2 4 or -log2 p\n",
    "  \n",
    "What is the amount information you'll get from the weather forecast on average ?\n",
    "\n",
    "    -0.25*log2(0.25)-0.75log2(0.75)=0.81\n",
    "\n",
    "This is entropy, in general the entropy is, \n",
    "    \n",
    "    `H(p)=-np.sum(p*np.log2(p))`\n",
    "\n",
    "It's a measure of average information received or the unpredictability of the probablity distribution\n",
    "\n",
    "Cross entropy refers to the entropy calculated using the true distribution and the predicted distribution :\n",
    "\n",
    "    `H(p,q)=-np.sum(p*np.log2(q))` - p - true distribution and q - predicted distribution\n",
    "    \n",
    "Why do we use it as a cost function? It's a guess, but H(p) = H(p,q) if p=q, when it's not H(p,q) > H(p), and the differenceis  called the kullback leibler divergence(KLD), and I read somewhere that KLD provides some sort of distance measure between the true and the predicted distribution.\n",
    "    \n",
    "https://www.youtube.com/watch?v=ErfnhcEV1O8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the parameters ( variables in tensorflow )\n",
    "\n",
    "We'd want to be able to save the parameters and then use them for predictions, so let's train the model and save it here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate=0.01\n",
    "batch_size=100\n",
    "display_step = 1\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,784], name='X')\n",
    "Y=tf.placeholder(dtype=tf.float32,shape=[None,10], name='Y')\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]),name='W')\n",
    "b= tf.Variable(tf.zeros([10]),name='b')\n",
    "\n",
    "Z=tf.matmul(X,W) +b\n",
    "A=tf.nn.softmax(Z)\n",
    "mnist = input_data.read_data_sets('data/mnist_data/',one_hot=True)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(A), axis=-1))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_index=0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, batch_cost = sess.run([train_step, cost], feed_dict={X:batch_xs,Y:batch_ys}) \n",
    "        batch_index +=1\n",
    "        #print(\"Batch:\", '%04d' % (batch_index), \"cost=\", \"{:.9f}\".format(batch_cost))                        \n",
    "            \n",
    "    print(\"Optimization Finished!\")     \n",
    "   \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(A, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    print(\"Train Accuracy:\", accuracy.eval({X: mnist.train.images, Y: mnist.train.labels}))\n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    saver = tf.train.Saver() \n",
    "    model_path=saver.save(sess, \"./tbout/models/model.ckpt\")\n",
    "    print ( 'Saving model to {}'.format(model_path) )\n",
    "    \n",
    "writer = tf.summary.FileWriter('./tbout/mnist2',sess.graph)\n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrieve it and use it for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "W = tf.get_variable(\"W\", shape=[784,10])\n",
    "b = tf.get_variable(\"b\", shape=[10])\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,784], name='X')\n",
    "Y=tf.placeholder(dtype=tf.float32,shape=[None,10], name='Y')\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver() \n",
    "    saver.restore(sess, \"./tbout/models/model.ckpt\")\n",
    "\n",
    "    print(b.eval())\n",
    "\n",
    "    Z=tf.matmul(X,W) +b\n",
    "    A=tf.nn.softmax(Z)\n",
    "\n",
    "    image=mnist.train.images[600]\n",
    "    result=sess.run(tf.argmax(A,axis=1),feed_dict={X:image.reshape(1,784)})\n",
    "    print(\"The recognized digit is {}\".format(result))\n",
    "    plt.imshow(image.reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
