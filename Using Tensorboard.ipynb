{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Name Scope\n",
    "\n",
    "## Basic Tensorboard Graph from mnist example 1 \n",
    "\n",
    "![alt text](initial.png \"Basic Tensorboard Graph\")\n",
    "\n",
    "Let's use some name scope to group the operations, in order to make the graph less cluttered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate=0.01\n",
    "batch_size=100\n",
    "display_step = 1\n",
    "mnist = input_data.read_data_sets('data/mnist_data/',one_hot=True)\n",
    "\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,784], name='X')\n",
    "Y=tf.placeholder(dtype=tf.float32,shape=[None,10], name='Y')\n",
    "\n",
    "with tf.name_scope('parameter_W_and_b_initialization'):\n",
    "    W = tf.Variable(tf.zeros([784,10]),name='W')\n",
    "    b= tf.Variable(tf.zeros([10]),name='b')\n",
    "\n",
    "with tf.name_scope('linear'):\n",
    "    Z=tf.matmul(X,W) +b\n",
    "\n",
    "with tf.name_scope('predict'):\n",
    "    A=tf.nn.softmax(Z)\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(A), axis=-1))\n",
    "\n",
    "with tf.name_scope('train'):    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    batch_index=0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    with tf.name_scope('fit_with_training_data'):\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, batch_cost = sess.run([train_step, cost], feed_dict={X:batch_xs,Y:batch_ys}) \n",
    "            batch_index +=1\n",
    "        #print(\"Batch:\", '%04d' % (batch_index), \"cost=\", \"{:.9f}\".format(batch_cost))                        \n",
    "            \n",
    "    print(\"Optimization Finished!\")     \n",
    "    print(str(W))\n",
    "    print(str(b))\n",
    "    \n",
    "    with tf.name_scope('calculate_accuracy'):\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(A, 1), tf.argmax(Y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print(\"Train Accuracy:\", accuracy.eval({X: mnist.train.images, Y: mnist.train.labels}))\n",
    "        print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "    with tf.name_scope('single_image_test'):\n",
    "        image=mnist.train.images[600]\n",
    "        result=sess.run(tf.argmax(A,axis=1),feed_dict={X:image.reshape(1,784)})\n",
    "        print(\"The recognized digit is {}\".format(result))\n",
    "        plt.imshow(image.reshape(28,28))\n",
    "    \n",
    "writer = tf.summary.FileWriter('./tbout/mnistBetterLogging',sess.graph)\n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement using named scope\n",
    "\n",
    "![alt text](namescope.png \"Improve Graph using name scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scalar and Histogram Summary\n",
    "\n",
    "In your program include code like this\n",
    "\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.histogram('weights',W)\n",
    "    tf.summary.histogram('biases', b)\n",
    "    tf.summary.histogram('activations', A)\n",
    "    merge_summary = tf.summary.merge_all()\n",
    "    training_writer = tf.summary.FileWriter('./tbout/mnistWithScalarAndHistogram',sess.graph)\n",
    "    \n",
    "    \n",
    "Run the merge_summary operation in the session like this -  \n",
    "\n",
    "    summary,batch_accuracy,batch_cost=sess.run([merge_summary,accuracy,cost],\n",
    "                                                   feed_dict={X:batch_xs,Y:batch_ys},\n",
    "                                                   options=run_options,\n",
    "                                                   run_metadata=run_metadata)     \n",
    "\n",
    "Call the writer to add the summary \n",
    "\n",
    "    training_writer.add_summary(summary, i)                       \n",
    "    \n",
    "    \n",
    "Note : You will get funny errors like if you attempt to run it multiple times, click Kernel->Restart and clear output\n",
    "\n",
    "    InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Inputs' with dtype float and shape [?,784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate=0.01\n",
    "batch_size=100\n",
    "display_step = 1\n",
    "mnist = input_data.read_data_sets('data/mnist_data/',one_hot=True)\n",
    "\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[None,784], name='Inputs')\n",
    "Y=tf.placeholder(dtype=tf.float32,shape=[None,10], name='Labels')\n",
    "\n",
    "with tf.name_scope('param_initialization'):\n",
    "    W = tf.Variable(tf.zeros([784,10]),name='W')\n",
    "    b= tf.Variable(tf.zeros([10]),name='b')\n",
    "\n",
    "with tf.name_scope('linear'):\n",
    "    Z=tf.matmul(X,W) +b\n",
    "\n",
    "with tf.name_scope('predict'):\n",
    "    A=tf.nn.softmax(Z)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(A, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            \n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(A), axis=-1))\n",
    "\n",
    "with tf.name_scope('train'):    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    tf.summary.scalar('accuracy',accuracy)\n",
    "    tf.summary.histogram('weights',W)\n",
    "    tf.summary.histogram('biases', b)\n",
    "    tf.summary.histogram('activations', A)\n",
    "    merge_summary = tf.summary.merge_all()\n",
    "    training_writer = tf.summary.FileWriter('./tbout/mnistWithScalarAndHistogram',sess.graph)\n",
    "    batch_index=0\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        sess.run(train_step, feed_dict={X:batch_xs,Y:batch_ys})\n",
    "\n",
    "        summary,batch_accuracy,batch_cost=sess.run([merge_summary,accuracy,cost],\n",
    "                                                   feed_dict={X:batch_xs,Y:batch_ys},\n",
    "                                                   options=run_options,\n",
    "                                                   run_metadata=run_metadata) \n",
    "        training_writer.add_summary(summary, i)\n",
    "        batch_index +=1\n",
    "            \n",
    "    print(\"Optimization Finished!\")     \n",
    "\n",
    "\n",
    "training_writer.flush()\n",
    "training_writer.close()   \n",
    "print(\"tensorboard --logdir=/notebooks/tbout/mnistWithScalarAndHistogram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how tensorboard will look like now \n",
    "\n",
    "![alt text](scalar.png \"Changes in Cost and accuracy ( Scalar ) \")\n",
    "\n",
    "![alt text](DistributionAndHistogram.png \"Weights, activation and  biases distribution \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
